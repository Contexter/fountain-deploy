To create a script that is more flexible and not hardcoded to the "Playwright" resource, let's adapt the script to use variables and read input for generating resources dynamically. This approach allows you to specify different model names and attributes without editing the script for each new resource. 

Below is an example of how we could modify the script to accept input for a resource name and its fields, making it reusable for any model you want to create.

### Dynamic Resource Generation Script

This script prompts the user for a resource name and attributes, generating the necessary files for a FastAPI project including SQLAlchemy models, Pydantic schemas, CRUD operations, and API routes. 

```bash
#!/bin/bash

echo "Enter resource name (e.g., Playwright):"
read ResourceName

# Convert ResourceName to lowercase for file names
resource_name=$(echo "$ResourceName" | awk '{print tolower($0)}')

echo "Enter space-separated fields (e.g., 'name:str bio:text'): "
read -a fields

# Directory paths
PROJECT_ROOT=$(pwd)
APP_DIR="$PROJECT_ROOT/app"
DB_DIR="$APP_DIR/db"
MODELS_DIR="$DB_DIR/models"
SCHEMAS_DIR="$APP_DIR/schemas"
SERVICES_DIR="$APP_DIR/services"
API_ENDPOINTS_DIR="$APP_DIR/api/routes"

# Create necessary directories
mkdir -p $MODELS_DIR $SCHEMAS_DIR $SERVICES_DIR $API_ENDPOINTS_DIR

# Start generating files
# SQLAlchemy Model
model_path="$MODELS_DIR/${resource_name}.py"
echo "from sqlalchemy import Column, Integer, String, Text" > $model_path
echo "from database import Base" >> $model_path
echo "" >> $model_path
echo "class $ResourceName(Base):" >> $model_path
echo "    __tablename__ = '${resource_name}'" >> $model_path
echo "    id = Column(Integer, primary_key=True, autoincrement=True)" >> $model_path

for field in "${fields[@]}"
do
    IFS=':' read -r name type <<< "$field"
    echo "    $name = Column($type)" >> $model_path
done

# Pydantic Schema
schema_path="$SCHEMAS_DIR/${resource_name}.py"
{
    echo "from pydantic import BaseModel"
    echo ""
    echo "class ${ResourceName}Base(BaseModel):"
} > $schema_path
for field in "${fields[@]}"
do
    IFS=':' read -r name type <<< "$field"
    echo "    $name: $type" >> $schema_path
done
{
    echo ""
    echo "class ${ResourceName}Create(${ResourceName}Base):"
    echo "    pass"
    echo ""
    echo "class ${ResourceName}InDB(${ResourceName}Base):"
    echo "    id: int"
    echo "    class Config:"
    echo "        orm_mode = True"
} >> $schema_path

# CRUD Operations
crud_path="$SERVICES_DIR/crud_${resource_name}.py"
{
    echo "from sqlalchemy.orm import Session"
    echo "from .models.${resource_name} import $ResourceName"
    echo ""
    echo "def get_${resource_name}(db: Session, ${resource_name}_id: int):"
    echo "    return db.query($ResourceName).filter($ResourceName.id == ${resource_name}_id).first()"
    echo ""
    echo "def create_${resource_name}(db: Session, ${resource_name}):"
    echo "    db_${resource_name} = $ResourceName(**${resource_name}.dict())"
    echo "    db.add(db_${resource_name})"
    echo "    db.commit()"
    echo "    db.refresh(db_${resource_name})"
    echo "    return db_${resource_name}"
} > $crud_path

# API Routes
api_path="$API_ENDPOINTS_DIR/${resource_name}.py"
{
    echo "from fastapi import APIRouter, Depends"
    echo "from sqlalchemy.orm import Session"
    echo "from ..schemas.${resource_name} import ${ResourceName}Create, ${ResourceName}InDB"
    echo "from ..services.crud_${resource_name} import get_${resource_name}, create_${resource_name}"
    echo "from ..dependencies import get_db"
    echo ""
    echo "router = APIRouter()"
    echo ""
    echo "@router.post('/${resource_name}/', response_model=${ResourceName}InDB)"
    echo "def create_${resource_name}_endpoint(${resource_name}: ${ResourceName}Create, db: Session = Depends(get_db)):"
    echo "    return create_${resource_name}(db=db, ${resource_name}=${resource_name})"
    echo ""
    echo "@router.get('/${resource_name}/{${resource_name}_id}', response_model=${ResourceName}InDB)"
    echo "def get_${resource_name}_endpoint(${resource_name}_id: int, db: Session = Depends(get_db)):"
    echo "    db_${resource_name} = get_${resource_name}(db=db, ${resource_name}_id=${resource_name}_id)"
    echo "    if db_${resource_name} is None:"
    echo "        raise HTTPException(status_code=404, detail='${ResourceName} not found')"
    echo "    return db_${resource_name}"
} > $api_path

echo "${ResourceName} resource files generated successfully."
```

### Usage

- Save this script in your project's root directory.
- Make it executable with `chmod +x script_name.sh`.
- Run the script by typing `./script_name.sh`.
- Follow the prompts to enter the resource name and fields.

This script dynamically generates a SQLAlchemy model, Pydantic schema, CRUD operations, and API routes based on your input, making it flexible for creating any resource in your FastAPI project.

### a quick input tutorial 

on how to set variables to input into the script;

once we had a big bunch bootstrap script that look like this:

```
-- Fountain Database Bootstrap for Microservices Architecture

/* 
Reason for change:
Moving from a monolithic structure with tight coupling via foreign keys to a microservices architecture.
This shift enhances scalability, flexibility, and service isolation, allowing each part of the play or screenplay
to be managed by dedicated microservices. Using SERIAL PRIMARY KEY in each table ensures uniqueness and independence,
aligning with the principles of microservices by eliminating direct dependencies between tables.
*/

-- Playwright Table
CREATE TABLE Playwright (
    Author_ID SERIAL PRIMARY KEY,
    Name TEXT NOT NULL,
    Biography TEXT,
    Contact_Information TEXT
);

-- Metadata Table
CREATE TABLE Metadata (
    Metadata_ID SERIAL PRIMARY KEY,
    Creation_Date DATE,
    Last_Modified_Date DATE,
    Version_Number INTEGER,
    Additional_Information TEXT
);

-- Script Table
CREATE TABLE Script (
    Script_ID SERIAL PRIMARY KEY,
    Title TEXT NOT NULL,
    Author_ID INTEGER,
    URL TEXT,
    Metadata_ID INTEGER
);

-- Act Table
CREATE TABLE Act (
    Act_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Act_Number INTEGER NOT NULL,
    Synopsis TEXT,
    Notes TEXT
);

-- Scene Table
CREATE TABLE Scene (
    Scene_ID SERIAL PRIMARY KEY,
    Act_ID INTEGER,
    Scene_Number INTEGER NOT NULL,
    Synopsis TEXT,
    Notes TEXT
);

-- Character Table
CREATE TABLE Character (
    Character_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Name TEXT NOT NULL,
    Description TEXT
);

-- Dialogue Table
CREATE TABLE Dialogue (
    Dialogue_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Character_ID INTEGER,
    Original_Text TEXT,
    Modernized_Text TEXT
);

-- Action Table
CREATE TABLE Action (
    Action_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Character_ID INTEGER,
    Original_Text TEXT,
    Modernized_Text TEXT
);

-- Transition Table
CREATE TABLE Transition (
    Transition_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Transition_Text TEXT
);

-- Parenthetical Table
CREATE TABLE Parenthetical (
    Parenthetical_ID SERIAL PRIMARY KEY,
    Dialogue_ID INTEGER,
    Original_Text TEXT,
    Modernized_Text TEXT
);

-- Note Table
CREATE TABLE Note (
    Note_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Type TEXT,
    Text TEXT
);

-- CenteredText Table
CREATE TABLE CenteredText (
    Centered_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Text TEXT
);

-- PageBreak Table
CREATE TABLE PageBreak (
    Page_Break_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Page_Number INTEGER
);

-- SectionHeading Table
CREATE TABLE SectionHeading (
    Section_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Text TEXT
);

-- TitlePage Table
CREATE TABLE TitlePage (
    Title_Page_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Text TEXT
);

-- Casting Table
CREATE TABLE Casting (
    Casting_ID SERIAL PRIMARY KEY,
    Character_ID INTEGER,
    Actor_Characteristics_Choices TEXT
);

-- CharacterRelationship Table
CREATE TABLE CharacterRelationship (
    Relationship_ID SERIAL PRIMARY KEY,
    Character1_ID INTEGER,
    Character2_ID INTEGER,
    Relationship_Type TEXT
);

-- MusicSound Table
CREATE TABLE MusicSound (
    Music_Sound_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Cue TEXT,
    Description TEXT
);

-- Props Table
CREATE TABLE Props (
    Prop_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Description TEXT
);

-- RevisionHistory Table
CREATE TABLE RevisionHistory (
    Revision_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Date DATE,
    Change_Description TEXT,
    Editor TEXT
);

-- FormattingRules Table
CREATE TABLE FormattingRules (
    Rule_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Rule_Description TEXT
);

-- CrossReferences Table
CREATE TABLE CrossReferences (
    Cross_Reference_ID SERIAL PRIMARY KEY,
    Scene_ID INTEGER,
    Referenced_Scene_ID INTEGER,
    Description TEXT
);

-- ExtendedNotesResearch Table
CREATE TABLE ExtendedNotesResearch (
    Research_ID SERIAL PRIMARY KEY,
    Script_ID INTEGER,
    Notes TEXT,
    Research_Details TEXT
);

-- SceneLocation Table
CREATE TABLE SceneLocation (
    Location_ID SERIAL PRIMARY KEY,
    Description TEXT NOT NULL,
    Historical_Cultural_Significance TEXT
);

/*
Each table is designed as a microservice, capable of operating independently. This architecture choice promises enhanced scalability, resilience, and flexibility in managing and evolving the fountain backend. By employing SERIAL PRIMARY KEYS, each entity within the ecosystem maintains its uniqueness and integrity, facilitating seamless interactions within and across services without the need for direct database relationships.
*/
```
To transition from a SQL schema to using the dynamic script for generating FastAPI components, we'll first need to interpret your SQL table definitions into a format that the script can understand. This tutorial will guide you through preparing input for the dynamic script based on the provided SQL schema example, focusing on creating FastAPI components for each table.

### Understanding the Input Format

The script prompts for two pieces of information:
1. **Resource Name**: This is the name of your table or entity, formatted in CamelCase for the Python class name (e.g., Playwright, Metadata, Script).
2. **Space-separated fields**: Each field should be defined by its name followed by a colon and its data type. Supported data types in the script are basic types that can be translated into SQLAlchemy and Pydantic types (e.g., `name:str`, `biography:text`).

### Example: Playwright Table

Given the SQL definition:

```sql
CREATE TABLE Playwright (
    Author_ID SERIAL PRIMARY KEY,
    Name TEXT NOT NULL,
    Biography TEXT,
    Contact_Information TEXT
);
```

You would input:

- **Resource Name**: `Playwright`
- **Fields**: 
    - `author_id:int` (assuming `SERIAL` translates to an auto-increment integer)
    - `name:str`
    - `biography:text`
    - `contact_information:text`

### Preparing Your Input

1. **List Your Tables**: Start with listing out all the tables you need to create FastAPI components for.
    - Playwright
    - Metadata
    - Script
    - ... (Continue for all tables)

2. **Translate SQL to Script Input**: For each table, write down the resource name and its fields according to the script's expected input format.

    Example for another table, `Metadata`:
    - **Resource Name**: `Metadata`
    - **Fields**: `metadata_id:int creation_date:date last_modified_date:date version_number:int additional_information:text`

### Writing the Input for the Script

When you run the script, it will first ask for the resource name. You will input the name of the table (e.g., `Playwright`). Next, it asks for the space-separated fields. Here, you provide the fields as interpreted from your SQL schema (e.g., `author_id:int name:str biography:text contact_information:text`).

Repeat this process for each table you have in your SQL schema.

### Automating Multiple Tables

If you aim to automate this process for multiple tables, you might consider extending the script to read from a configuration file or accept command-line arguments for batch processing tables. However, the initial version of the script is interactive and designed for processing one resource at a time to ensure accuracy and allow for custom input per resource.

### Final Note

This approach requires a bit of manual preparation to translate your SQL schema into the input format for the script. The benefit is that once you've prepared your inputs, generating the FastAPI components becomes a quick and repeatable process for any table in your schema, facilitating a smoother transition to a microservices architecture.

### But wait - Oh NO! There's something missing !

it is devastating: for reasons unknown the alembic part of things - this last step than actually generates the database table - is missing from the generator : so let's fix this and hope for the best: here its THE SCRIPT:

This will include initializing Alembic (if not already done), generating a new migration script for the SQLAlchemy model you've created, and applying this migration to update your database schema accordingly.

### Extended Script with Alembic Integration

```
bash
#!/bin/bash

echo "Enter resource name (e.g., Playwright):"
read ResourceName

# Convert ResourceName to lowercase for file names
resource_name=$(echo "$ResourceName" | awk '{print tolower($0)}')

echo "Enter space-separated fields (e.g., 'name:str bio:text'): "
read -a fields

# Directory paths
PROJECT_ROOT=$(pwd)
APP_DIR="$PROJECT_ROOT/app"
DB_DIR="$APP_DIR/db"
MODELS_DIR="$DB_DIR/models"
SCHEMAS_DIR="$APP_DIR/schemas"
SERVICES_DIR="$APP_DIR/services"
API_ENDPOINTS_DIR="$APP_DIR/api/routes"

# Create necessary directories
mkdir -p $MODELS_DIR $SCHEMAS_DIR $SERVICES_DIR $API_ENDPOINTS_DIR

# Start generating files
# SQLAlchemy Model
model_path="$MODELS_DIR/${resource_name}.py"
echo "from sqlalchemy import Column, Integer, String, Text" > $model_path
echo "from database import Base" >> $model_path
echo "" >> $model_path
echo "class $ResourceName(Base):" >> $model_path
echo "    __tablename__ = '${resource_name}'" >> $model_path
echo "    id = Column(Integer, primary_key=True, autoincrement=True)" >> $model_path

for field in "${fields[@]}"
do
    IFS=':' read -r name type <<< "$field"
    echo "    $name = Column($type)" >> $model_path
done

# Pydantic Schema
schema_path="$SCHEMAS_DIR/${resource_name}.py"
{
    echo "from pydantic import BaseModel"
    echo ""
    echo "class ${ResourceName}Base(BaseModel):"
} > $schema_path
for field in "${fields[@]}"
do
    IFS=':' read -r name type <<< "$field"
    echo "    $name: $type" >> $schema_path
done
{
    echo ""
    echo "class ${ResourceName}Create(${ResourceName}Base):"
    echo "    pass"
    echo ""
    echo "class ${ResourceName}InDB(${ResourceName}Base):"
    echo "    id: int"
    echo "    class Config:"
    echo "        orm_mode = True"
} >> $schema_path

# CRUD Operations
crud_path="$SERVICES_DIR/crud_${resource_name}.py"
{
    echo "from sqlalchemy.orm import Session"
    echo "from .models.${resource_name} import $ResourceName"
    echo ""
    echo "def get_${resource_name}(db: Session, ${resource_name}_id: int):"
    echo "    return db.query($ResourceName).filter($ResourceName.id == ${resource_name}_id).first()"
    echo ""
    echo "def create_${resource_name}(db: Session, ${resource_name}):"
    echo "    db_${resource_name} = $ResourceName(**${resource_name}.dict())"
    echo "    db.add(db_${resource_name})"
    echo "    db.commit()"
    echo "    db.refresh(db_${resource_name})"
    echo "    return db_${resource_name}"
} > $crud_path

# API Routes
api_path="$API_ENDPOINTS_DIR/${resource_name}.py"
{
    echo "from fastapi import APIRouter, Depends"
    echo "from sqlalchemy.orm import Session"
    echo "from ..schemas.${resource_name} import ${ResourceName}Create, ${ResourceName}InDB"
    echo "from ..services.crud_${resource_name} import get_${resource_name}, create_${resource_name}"
    echo "from ..dependencies import get_db"
    echo ""
    echo "router = APIRouter()"
    echo ""
    echo "@router.post('/${resource_name}/', response_model=${ResourceName}InDB)"
    echo "def create_${resource_name}_endpoint(${resource_name}: ${ResourceName}Create, db: Session = Depends(get_db)):"
    echo "    return create_${resource_name}(db=db, ${resource_name}=${resource_name})"
    echo ""
    echo "@router.get('/${resource_name}/{${resource_name}_id}', response_model=${ResourceName}InDB)"
    echo "def get_${resource_name}_endpoint(${resource_name}_id: int, db: Session = Depends(get_db)):"
    echo "    db_${resource_name} = get_${resource_name}(db=db, ${resource_name}_id=${resource_name}_id)"
    echo "    if db_${resource_name} is None:"
    echo "        raise HTTPException(status_code=404, detail='${ResourceName} not found')"
    echo "    return db_${resource_name}"
} > $api_path

echo "${ResourceName} resource files generated successfully."

# Alembic integration for migrations
ALEMBIC_DIR="$PROJECT_ROOT/alembic"
if [ ! -d "$ALEMBIC_DIR" ]; then
    alembic init alembic
fi

# Update Alembic's env.py to dynamically discover models
echo "import os
import sys
sys.path.insert(0, os.path.realpath(os.path.join(os.path.dirname(__file__), '..')))
from app.db.base import Base  # Ensure this import matches your base model setup
from app.db.models import *  # This assumes all models are imported here
target_metadata = Base.metadata" >> $ALEMBIC_DIR/env.py

# Generate and apply migrations
alembic revision --autogenerate -m "Add ${ResourceName} model"
alembic upgrade head

echo "Alembic migrations for ${ResourceName} generated and applied."
```

This script combines all the steps: it prompts for a resource name and fields, creates the necessary directories, generates SQLAlchemy models, Pydantic schemas, CRUD operations, API routes, and integrates with Alembic for database migrations. It's designed to be run in a Bash-compatible shell within the root directory of a FastAPI project structured as you've described.

Ensure Alembic and all necessary Python packages (`fastapi`, `sqlalchemy`, `alembic`, `pydantic`) are installed in your environment before running the script. The path to `app.db.base` and `app.db.models` in the `env.py` update step may need adjustment based on your project's actual structure.

